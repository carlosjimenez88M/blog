---
title: "Un caso aplicado a los metódos de ensamble"
output: html_document
mathjax: "true"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, cache=FALSE,message=FALSE, warning=FALSE}
library(tidyverse)
library(randomForest)
library(tree)
library(gbm)
library(lubridate)
library(rpart)
library(rpart.plot)
theme_set(theme_light())
```

## Metódos de ensamble

Uno de los problemas al trabajar con `machine learning` consiste en optimizar el `accuracy` de los modelos y para ello una de las mejores vias es trabajar con modelos de ensamble, que en si consiste en mejorar la precisión a través del uso de diferentes algoritmos.

En este post trabajare con dichos metódos a través de un ejemplo, pero a la par incluire un concepto estadístico que a mi pensar hace el combo perfecto para el desarrollo bajo está metodología, hablaré por lo tanto de **VIF**.


## Conceptos previsos

Para hablar de metódos de ensamble es necesario aclarar varios algoritmos para trabajar , en el siguiente apartado trataré con los árboles de decisión.

### Árboles de decisión 

Una definición acertada que se da desde el portal de [towards data science](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052) es la siguiente 


> "A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification and regression. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. "


Uno de los aspectos más importante para trabajar con este tipo de algoritmos es entender que el segmenta los espacios de los predictores en $n$ regiones a través del promedio o la mediana de los datos. Por lo tanto estás regiones tienden a ser independientes unas de otras y puede que sus dimensiones sean diferentes entre si , generando un [rmse](https://www.statisticshowto.datasciencecentral.com/rmse/) lo suficientemente bajo, de la siguiente manera


$$
\sum_{j=1}^{J} \sum_{i \in R_{j}}(y_{i}-\hat{y_{R_j}})^2 \mbox{ Donde Y_jr}   \mbox{ es el promedio de la región respuesta de } J
$$

Más hay que tener en cuenta que al comienzo de cada árbol todos los datos empiezan en la misma región $J$ y en cada stepwise se divide en dos, buscando los mejores parámetros a través de una división recursiva.


Por lo tanto teniendo en cuenta las regiones y los predictores se generan los cortes a través de una minimización de la suma de los RSS

$$
\sum_{i:x_{i}\in R_{1}(a,j)} (y_{i}-\hat{y}_{r_i})^2+\sum_{i:x_{i}\in R_{2}(a,j)} (y_{i}-\hat{y}_{r_i})^2
$$


Para determinar el tamaño de estos árboles es necesario hacer una reducción de la varianza a través de arboles más pequeños (bias) con lo  cual se mejora la capacidad de interpretación de los mismos y de paso así un menor error , pero esto a través de cost complexity pruning , en donde se selecciona un número de sub-árboles de interés.

### Bagging 

Es una técnica creada por Leo Breiman (2005), que consiste Boostrap aggregation que permite estabilizar el accuracy de un modelo , gracias a que genera diferentes modelos gracias a que genera muestras aleatorias con reemplazo y luego para cada una de ellas busca el modelo más adecuado para resolver cada  región de datos. 

Por lo anterior, se puede reducir el tamaño de la varianza $\sigma^2$ promediando el tamaño de las observaciones a través de $(N_{(1,2,3,...,z)}/\sigma^2)^{-1}$, para asi después validar el modelo a travñes de boostrap con un $n$ del 67% de las observaciones. EL resto de los datos se les conoce como *out of bag* se usa para calcular nuevas predicciones.

### Random Forest 

Es un algoritmo de ML supervisado en donde se generan multiples árboles de decisiones en donde por cada una de sus ramificaciones suele trabajar con un predictor , lo cual reduce en gran medida la varianza y no genera un predictor único

### Boosting

Funciona igual que el bagging en el decision tree, pero en este caso cada nodo se construye con información residual , por lo cual su dimensión tiende a ser más pequeña. 

## Métodos de ensamble 

En está sección se presentará un ejemplo de como generar ensambles , se trabajará con el set de datos de `nobels`


```{r, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
nobel_winners <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-14/nobel_winners.csv") %>%
  distinct(full_name, prize_year, category, .keep_all = TRUE) %>%
  mutate(decade = 10 * (prize_year %/% 10),
         age = prize_year - year(birth_date))

library(knitr)

nobel_winners%>%
  head()%>%
  kable(format = 'pandoc',caption = 'Nobels Prize')
```


Un poco de análisis exploratorio , en este paso se intentará describir el número de premios nobel por áreas.


```{r, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
nobel_winner_all_pubs <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-14/nobel_winner_all_pubs.csv") %>%
  mutate(prize_decade = 10 * (prize_year %/% 10))


nobel_winners %>%
  group_by(category, decade) %>%
  summarize(winners = n(),
            winners_per_year = winners / n_distinct(prize_year)) %>%
  ggplot(aes(decade, winners_per_year, fill = category)) +
  geom_col() +
  geom_text(aes(label=round(winners_per_year,0)),position = position_stack(vjust = .5))+
  expand_limits(y = 0)+
  coord_flip()+
  labs(title = 'Nobel Price count by Decade')
```





```{r, echo=FALSE, cache=FALSE}
library(tidytext)
nobel_winners %>%
  count(decade,
        category,
        gender = coalesce(gender, laureate_type)) %>%
  group_by(decade, category) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(decade, n, fill = gender)) +
  geom_col() +
  facet_wrap(~ category) +
  labs(x = "Decade",
       y = "# of nobel prize winners",
       fill = "Gender",
       title = "Nobel Prize gender distribution over time")
```
Ahora se evalua como es el comportamiento por pais y dada la categoría del nobel.

```{r, echo=FALSE, cache=FALSE}
nobel_winners %>%
  filter(!is.na(birth_country)) %>%
  count(birth_country = fct_lump(birth_country, 9),
        category,
        sort = TRUE) %>%
  mutate(birth_country = fct_reorder(birth_country, n)) %>%
  ungroup %>%
    mutate(category = as.factor(category),
           birth_country = reorder_within(birth_country, n, category)) %>%
  ggplot(aes(birth_country, n, fill = category)) +
  geom_col() +
  guides(fill=FALSE)+
  facet_wrap(~ category, scales = 'free') +
  scale_x_reordered() +
    scale_y_continuous(expand = c(0,0))+
  coord_flip()
```
Se genera un featuring en donde se explora el tipo de pais a través de ingresos para asi clasificar por rangos el tipo de pasís (a nivel económico) y sus caracteristicas en los nobels 
```{r, echo=FALSE, cache=FALSE}
library(WDI)
library(countrycode)
indicators_raw <- WDI(indicator = "NY.GDP.PCAP.CD",
                      start = 2016, end = 2016, extra = TRUE) %>%
  tbl_df() %>%
  select(country,
         country_code = iso2c,
         income,
         gdp_per_capita = NY.GDP.PCAP.CD)

```




```{r, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
nobel_winners_countries <- nobel_winners %>%
  mutate(country_code = countrycode(birth_country, "country.name", "iso2c")) %>%
  inner_join(indicators_raw, by = "country_code") %>%
  mutate(income = fct_relevel(income, c("Low income", "Lower middle income", "Upper middle income", "High income")))

```


Featuring tomado de [David Robinson](http://varianceexplained.org/about/)
```{r, echo=FALSE, cache=FALSE}
nobel_winners_countries %>%
  filter(!is.na(income)) %>%
  count(category, income) %>%
  group_by(category) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(income, percent,fill=category)) +
  geom_col() +
  guides(fill=FALSE)+
  facet_wrap(~ category) +
  coord_flip() +
  labs(x = "Current income level of birth country",
       y = "% of this category's prizes",
       title = "Where do Nobel Prize winners come from?")
```






```{r}

nobel_winners_countries%>%
  group_by(category)%>%
  summarize(n=n())%>%
  arrange(desc(n))%>%
  mutate(category=fct_reorder(category,n))%>%
  ggplot(aes(category,n,fill=category))+
  geom_bar(stat ='identity')+
  labs(x='Categoría',y='Count',title = 'Relación de los premios por categoría')

```

Se crea la partición de los datos 

```{r}
library(caret)
nobel_winners_countries%>%
  na.omit()->data
index<-createDataPartition(data$category,times = 1,p = 0.8,list = FALSE)
train_set<-data[index,]
test_set<-data[-index,]

```



```{r}
prop.table(table(train_set$category))*100
```

Hay que hacer una transformación antes de implementar el modelo, y es convertir a factor la variable `category`

```{r}
test_set$category<-as.factor(test_set$category)
train_set$prize<-as.factor(train_set$prize)
train_set$gender<-as.factor(train_set$gender)
test_set$category<-as.factor(test_set$category)
test_set$prize<-as.factor(test_set$prize)
test_set$gender<-as.factor(test_set$gender)
train_set$category<-as.factor(train_set$category)
train_set$laureate_type<-as.factor(train_set$laureate_type)
test_set$laureate_type<-as.factor(test_set$laureate_type)
# Training with classification tree
multiclass <- rpart(category ~prize_year+age+gender+decade+income+gdp_per_capita+laureate_type, data=na.omit(train_set))
print(multiclass, digits = 3)

```


```{r}
rpart.plot(multiclass)
```



```{r}
predictions1 <- predict(multiclass, test_set, type = "class")
confusionMatrix(predictions1, test_set$category)

```

Ahora con el modelo de Randon Forest 
```{r}

library(randomForest)
set.seed(12345)

rf_mc <- randomForest(category ~prize_year+age+gender+decade+income+gdp_per_capita+laureate_type, data=na.omit(train_set))

# Predict the testing set with the trained model
predictions2 <- predict(rf_mc, test_set, type = "class")

# Accuracy and other metrics
confusionMatrix(predictions2, test_set$category)
```

Por el momento el árbol tiene mayor ventaja en el pronóstico





Con Bagging se encuentran los siguientes resultados
```{r}
seat_bag = randomForest(category ~prize_year+age+gender+decade+income+gdp_per_capita+laureate_type, data=na.omit(train_set), mtry = 6, importance = TRUE, ntrees = 50)
seat_bag


```

```{r}
predictions3 <- predict(seat_bag, test_set, type = "class")

# Accuracy and other metrics
confusionMatrix(predictions3, test_set$category)
```


La importancia de las variables acá es la siguiente 


```{r}
varImpPlot(seat_bag)
```

Por lo pronto se baja o poda el número de ramifiaciones para poder optimizar los parámetros
```{r}
oob = trainControl(method = "oob")
cv_10 = trainControl(method = "cv", number = 6)
rf_grid =  expand.grid(mtry = 1:6)
```


```{r}
set.seed(825)
seat_rf_tune = train(category ~prize_year+age+gender+decade+income+gdp_per_capita, data=na.omit(train_set),
                     method = "rf",
                     trControl = oob,
                     verbose = FALSE,
                     tuneGrid = rf_grid)
seat_rf_tune

```

Se identifico que el mejor modelo es con `mtry=3`, y su capacidad de pronóstico es la siguiente

```{r}
seat_bag_tst_pred = predict(seat_bag, newdata = test_set)
confusionMatrix(seat_bag_tst_pred, test_set$category)
```




```{r}
calc_acc = function(actual, predicted) {
  mean(actual == predicted)
}
(bag_tst_acc = calc_acc(predicted = seat_bag_tst_pred, actual = test_set$category))
```

Con boosting los resultados son los siguientes

```{r}
seat_boost = gbm(category ~prize_year+age+gender+decade+income+gdp_per_capita+laureate_type, data=na.omit(train_set), distribution = "tdist", 
                 n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
seat_boost
```




```{r}


seat_boost_tst_pred = predict(seat_boost, test_set, n.trees = 50)
(boost_tst_acc = calc_acc(predicted = seat_boost_tst_pred, actual = test_set$category))

```

Por lo tanto la selección del modelo será la siguiente 

```{r}
(seat_acc = data.frame(
  Model = c("Single Tree", "Random Forest",  "Bagging", "Boosting"),
  TestAccuracy = c(confusionMatrix(predictions1, test_set$category)$overall[1],confusionMatrix(predictions2, test_set$category)$overall[1],confusionMatrix(predictions3, test_set$category)$overall[1],boost_tst_acc)
  )
)
```



```{r}
seat_acc%>%
  ggplot(aes(reorder(Model,TestAccuracy),TestAccuracy, fill=Model))+
  geom_col(show.legend = FALSE)+
  coord_flip()+
  labs(x="models",y="Test Accuracy")
```
EL modelo que mejor predice es el del árbol de decision, y por lo tanto este deberá ser el seleccionado para este trabajo

## Bibliografía

* https://daviddalpiaz.github.io/r4sl/ensemble-methods.html#classification-2

* https://rpubs.com/Cristina_Gil/arboles_ensemble

* https://moderndive.com/10-inference-for-regression.html

