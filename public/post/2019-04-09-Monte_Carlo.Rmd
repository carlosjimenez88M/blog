---
title: "Monte Carlo"
date: "9/4/2019"
output: html_document
mathjax: "true"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> "It has long been an axiom of mine that the little things are infinitely
the most important." Arthur Conan Doyle

```{r, echo=FALSE, cache=FALSE}
set.seed(1234)
montecarlo1=function(g,a,b,n)
{
  x=runif(n)
  rango=seq(from=1, to=n, by=1)
  #valoresg=apply(,g)
  valoresg=g((b-a)*x+a)
  s=c()
  for (i in rango) {
    s[i]=(b-a)*sum(valoresg[1:i])/i
  }
  s
  plot(s, type="l",xlab="Observaciones",ylab="Estimación integral",main="Monte Carlo para integrales de una variable",col="lightsalmon")
  
numerico=integrate(g, lower = a, upper = b)
lines(c(1,n),c(numerico[1],numerico[1]),col="green",lwd=2)
}

g=function(x)
{
  return((1/sqrt(2*pi))*exp(-(x^2)/2))
}
montecarlo1(g,-3.5,3.5,20000)
```



### ¿ En qué consiste los métodos de Monte Carlo ?

El nombre del método no proviene de un autor como estamos acostumbrados en el mundo de la estadística, sino de un casino en Mónaco, y puntualmente le dieron este titulo debido a que los juegos de azar que son *generadores de número aleatorios*, como los resultados de las cartas, la ruletas y demás.

De la mano con lo anterior y conforme al desarrollo de la computación, durante la Segunda Guerra Mundial (1944), se estudiaba problemas probabilisticos sobre la difusión de neutrones en hidrodinámicas donde la difusión posee un comportamiento plenamente aleatorio, por lo anterior sumado al  trabajo de [Neumann](https://es.wikipedia.org/wiki/John_von_Neumann) y [Metropolis](https://es.wikipedia.org/wiki/Nicholas_Metropolis) se logro estimar valores propios en la ecuación de Schrödinge a través de este método de simulación, esta simulación basada en Monte Carlo, dando como resultado que dadas $N$ muestras en un espacio de $M$ dimensiones se puede dar un resultado (o vectores resultantes) a pesar que el problema sea deterministico o estocástico.

Lo anterior hace refriega en un concepto claro sobre los limites en la inferencia estadística, los límites sobre los problemas numéricos, los cuales se presentan en dos casos:

* Problemas de optimización ;
* Problemas de integración.

Dichos problemas se presentan dado que no es posible calcular analíticamente los estimadores propios a un número de paradigmas. 

Dicho lo anterior, los mejores resultados para estos problemas vienen dados de generar simulaciones sustitutivas (de distribuciones o vectores), para calcular cantidades de interés, dado que se puede producir infinitos números de aleatorios acordes a una distribución que viene dada de un fenómeno frecuentista o asintóticos, lo cual resulta más fácil que la inferencia numérica, puesto que se puede obtener y controlar el tamaño de las muestras. Y usando los resultados probabilisticos a la ley de los grandes números o teoría del limite central, se puede evidenciar como convergen los métodos de simulación y así poder dar conclusiones sobre las observaciones.


Considere lo siguiente: Dada una variable aleatoria $X$, con función de densidad $f(x)$, se evalúa la expresión 

$$
\int_{x} h(x) f(x) dx
$$

Podría ser evaluada a través del método de Simpson, pero el área no se podría tratar con límites infinitos ó se puede evaluar a través de la generación de muestras $x_{1},...,x_{n}$ dada $f(x)$ y se aproxima a través de $\hat h_{n}=\frac{1}{n}\sum^{n}_{i=1} h(x_{i})$ (en otras palabras , promedio de los valores).


```{r, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```

Lo anterior se valida a través de la convergencia que viene dada por 

$$
\hat h_n =\frac{1}{n} \sum_{n}^{i=1} h(x_i) \rightarrow  \int_{x} h(x)  f(x) dx= \mathbf{E}_f [h(X)]
$$


Lo anterior se puede demostrar al calcular $\Gamma (\lambda)$ de la siguiente expresión, gracias a la ley de los grandes números que expresa que a una sucesión infinita de variables aleatorias independientes e idénticamente distribuidas que cumplen que $E(|x|< \infty)$ tiene un valor esperado en $\mu$ por lo cual el promedio de las variables aleatorias converge a $\mu$

$$
P(\lim_{n \rightarrow \infty} \hat{X}_n =\mu)=1
$$

Evaluando la expresión 

$$
\int_{0}^{\infty} x^{\lambda-1} exp(-x) dx
$$


El resultado es el siguiente

```{r, echo=FALSE, cache=FALSE}
ch=function(la){
integrate(function(x){x^(la-1)*exp(-x)},0,Inf)$val}
plot(lgamma(seq(.01,10,le=100)),log(apply(as.matrix(
 seq(.01,10,le=100)),1,ch)),xlab="log(integrate(f))",
 ylab=expression(log(Gamma(lambda))),pch=19,cex=.6)
```

Se puede evidenciar que no hay discrepancia en los valores pequeños de $\lambda$.

Más lo anterior tiene un problema y es que la integración falla a la hora de detectar el área de importancia de la función. Por lo tanto los métodos de simulación se enfocan en esta región explotando la información de la función de densidad asociada a la integral.

*Observación:* La función de integración puede aceptar infinitos límites, pero sus salidas no son confiables.


Consideremos el siguiente caso: Tengo una muestra de 15 elementos proveniente de una distribución Cauchy (la de la resonancia forzada) con parámetro de localización $\theta =500$. La psuedo-muestra margial del plano es 

$$
m(x)=\int_{-\infty}^{\infty} \prod^{15}_{i=1} \frac{1}{\pi} \frac{1}{1+(x_i - \theta)^2}d\theta
$$

Manteniendo la tesis que reza en la observación se puede ver que el valor que devuelve el resultado tiende a ser incorrecto


```{r, echo=FALSE, cache=FALSE}
cac=rcauchy(15)+500
lik=function(the){
u=dcauchy(cac[1]-the)
for (i in 2:15)
u=u*dcauchy(cac[i]-the)
 return(u)}
```



```{r}
integrate(lik,200,500) #función programada
```


Para evitar dicho problema del error , se recurre a Monte Carlo, donde el problema genérico es sobre la evaluación de 

$$
\mathbf{E}_{f}[h(X)] \int_{x} h(x) f(x)
$$


Donde x denota los valores de la variable aleatoria de X que son usuales o soportables (support) para la función de densidad $f(x)$, por lo tanto el principio teórico  de Monte Carlo está dado a la aproximación de la anterior función generando muestras dada la función de densidad y aproximaciones empíricas antes mencionada

$$
\hat{h}_n = \frac{1}{n} \sum_{n}^{i=1} h(x_i)
$$

Puesto que $h_n$ converge en $\mathbf{E}_{f}[h(X)]$ por la ley de los grandes números $P(\lim_{n \rightarrow \infty} \hat{X}_n =\mu)=1$

A demás cuando $h^2(x)$ tiene una expectativa finita sobre la función de densidad, la velocidad de convergencia en $h_n$ asciende a $O \sqrt(n)$ y la varianza asintótica se aproxima a la siguiente expresión 


$$
var(\hat{h}_n)=\frac{1}{n} \int_n (h(x)-\mathbf{E}_f [h(X)])^2 f(x) dx
$$

La cual también se puede estimar a través de la muestra de 

$$
v_n=\frac{1}{n^2} \sum_{i=1}^{n}[h(x_i)-\hat{h}_n]^2
$$

Se construye el intervalo de confianza.

Esto gracias al teorema del limite central para números grandes, donde sea $X_1,X_2,...,X_n$  un conjunto de valores independientes e idénticamente distribuidos con promedio $\mu$ y varianza $0<\theta^2<\infty$, se obtiene   que

$$
\hat{X_n}=\frac{1}{n}(X_1+....+X_n-1,+X_n)
$$

Entonces 

$$
\lim_{n\rightarrow \infty} Pr(\frac{\hat{X_n}-\mu)}{\theta \sqrt} \leq z) \mbox{~ Normal(0,1)} 
$$


Lo que quiere decir que   converge $\hat{h_n}$ en $E(h(x))$

$$
\frac{\hat{h_n}-E(h(x))}{\sqrt{v_n}} \mbox{~ N(0,1)} 
$$


Dicho lo anterior se calcula $h(x)=[\cos(40x)+\sin(30x)]^2$, en donde se generaran muestras independientes e idénticamente distribuidas al rededor $\int h(x)dx$ con $\h(U_i)/n$ donde $U_i$ son variables aleatorias


```{r, echo=FALSE, cache=FALSE}
h=function(x){(cos(40*x)+sin(30*x))^2}
par(mar=c(2,2,2,1),mfrow=c(2,1))
curve(h,xlab="Function",ylab="",lwd=2)
integrate(h,0,1)
```
```{r, echo=FALSE, cache=FALSE}
x=h(runif(10^4))
estint=cumsum(x)/(1:10^4)
esterr=sqrt(cumsum((x-estint)^2))/(1:10^4)
plot(estint, xlab="Mean and error range",type="l",lwd=2,ylim=mean(x)+20*c(-esterr[10^4],esterr[10^4]),ylab="")
lines(estint+2*esterr,col="gold",lwd=2)
lines(estint-2*esterr,col="gold",lwd=2)
```

Otra forma de verlo es la siguiente

Suponga que hay que integrar la función 

$$
h(x)=(\cos(10x)+0.6 \sin(4x))^4
$$

En el intervalo (-1,2)

```{r}
a<-0;
b<-1;
h1<- function (x) (cos(10 * x) + 0.6 *sin(4*x))^4;
n<-1000
u<-runif(n,a,b)
mean(h(u))* (b-a)
```

Al desarrollar la integral observese que 

```{r}
integrate(h1,a,b)
```

Al gráficar se obtiene que 
```{r, echo=FALSE, cache=FALSE}
ggplot(data.frame(x=0),aes(x))+
  stat_function(fun = h1)+
  xlim(a,b)+
  labs(title = 'Integral de h(x)')+
  theme_bw()
```

Ahora se evidencia la eficiencia de Monte Carlo a medida que aumenta el tamaño de la población $n$ junto a los intervalos de confianza

```{r}
n_max=10^3
x<-h1(runif(n_max,a,b))
h_bar<-cumsum(x)/(1:n_max)
se<-sqrt((cumsum(x-h_bar)^2))/(1:n_max)
plot(h_bar, type = "l")
lines(h_bar+1.96*se, col="green")
lines(h_bar-1.96*se, col="red")
```

Observese que las integrales pueden usarse desde que $n>500$


```{r}
n_sim<-100
n<-500
h_bar_mc<-NULL


for(i in 1:n_sim){
  x<-h1(runif(n,a,b))
  h_bar_mc[i]<-mean(x)
}

mean(h_bar_mc)

```


```{r}
h_bar_mc
```






## Bibliográfia 

* Robert C,"Introducing Monte Carlo Methods with R" (2009)
* Gutiérrez A, "Teoría Estadística" (2009)



